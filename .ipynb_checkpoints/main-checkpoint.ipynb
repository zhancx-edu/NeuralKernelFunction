{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b50224-f12a-4641-863c-c920b9e3d5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from config import DATASET_CONFIGS\n",
    "from utils import azimuthal_equidistant_projection, quadrilateral_area\n",
    "from models import KernelPointProcess\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd0671c-e57c-4aec-9b8c-9ed63eb7e80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:28:32,394 - INFO - SCEDC_30: auxiliary=1142, training=6815, validation=3135, testing=1898\n",
      "2025-07-23 18:28:36,178 - INFO - Model and session initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_add_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:28:36,185 - WARNING - Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_add_1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_add_2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:28:36,186 - WARNING - Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_add_2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_add.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:28:36,187 - WARNING - Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_add.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_gradients_1/time_in_1_nmlz/truediv_grad/Reshape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:28:36,188 - WARNING - Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_gradients_1/time_in_1_nmlz/truediv_grad/Reshape.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to temporal_kernel_network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:28:36,189 - WARNING - Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to temporal_kernel_network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to multiply.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:28:36,190 - WARNING - Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to multiply.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to kappa_network.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:28:36,191 - WARNING - Output {0} missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to kappa_network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6815 samples, validate on 3135 samples\n",
      "Epoch 1/1000\n",
      "6815/6815 [==============================] - 4s 578us/sample - loss: 436550.2254 - val_loss: 427824.9436\n",
      "Epoch 2/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 9.8177 - val_loss: 9.5404\n",
      "Epoch 3/1000\n",
      "6815/6815 [==============================] - 2s 310us/sample - loss: 9.2853 - val_loss: 9.2531\n",
      "Epoch 4/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 9.0366 - val_loss: 8.6467\n",
      "Epoch 5/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.3572 - val_loss: 8.3106\n",
      "Epoch 6/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.1829 - val_loss: 8.1923\n",
      "Epoch 7/1000\n",
      "6815/6815 [==============================] - 2s 315us/sample - loss: 8.1379 - val_loss: 8.2072\n",
      "Epoch 8/1000\n",
      "6815/6815 [==============================] - 2s 304us/sample - loss: 8.1178 - val_loss: 8.1809\n",
      "Epoch 9/1000\n",
      "6815/6815 [==============================] - 2s 309us/sample - loss: 8.1171 - val_loss: 8.1477\n",
      "Epoch 10/1000\n",
      "6815/6815 [==============================] - 2s 310us/sample - loss: 8.1071 - val_loss: 8.1428\n",
      "Epoch 11/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.0940 - val_loss: 8.1301\n",
      "Epoch 12/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0881 - val_loss: 8.1433\n",
      "Epoch 13/1000\n",
      "6815/6815 [==============================] - 2s 308us/sample - loss: 8.0888 - val_loss: 8.1452\n",
      "Epoch 14/1000\n",
      "6815/6815 [==============================] - 2s 310us/sample - loss: 8.0841 - val_loss: 8.1294\n",
      "Epoch 15/1000\n",
      "6815/6815 [==============================] - 2s 314us/sample - loss: 8.0862 - val_loss: 8.1328\n",
      "Epoch 16/1000\n",
      "6815/6815 [==============================] - 2s 315us/sample - loss: 8.0838 - val_loss: 8.1276\n",
      "Epoch 17/1000\n",
      "6815/6815 [==============================] - 2s 315us/sample - loss: 8.0764 - val_loss: 8.1546\n",
      "Epoch 18/1000\n",
      "6815/6815 [==============================] - 2s 312us/sample - loss: 8.0814 - val_loss: 8.1167\n",
      "Epoch 19/1000\n",
      "6815/6815 [==============================] - 2s 317us/sample - loss: 8.0796 - val_loss: 8.1414\n",
      "Epoch 20/1000\n",
      "6815/6815 [==============================] - 2s 325us/sample - loss: 8.0898 - val_loss: 8.1271\n",
      "Epoch 21/1000\n",
      "6815/6815 [==============================] - 2s 310us/sample - loss: 8.0786 - val_loss: 8.1344\n",
      "Epoch 22/1000\n",
      "6815/6815 [==============================] - 2s 317us/sample - loss: 8.0829 - val_loss: 8.1328\n",
      "Epoch 23/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0767 - val_loss: 8.1217\n",
      "Epoch 24/1000\n",
      "6815/6815 [==============================] - 2s 310us/sample - loss: 8.0816 - val_loss: 8.1327\n",
      "Epoch 25/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.0822 - val_loss: 8.1380\n",
      "Epoch 26/1000\n",
      "6815/6815 [==============================] - 2s 315us/sample - loss: 8.0778 - val_loss: 8.1147\n",
      "Epoch 27/1000\n",
      "6815/6815 [==============================] - 2s 308us/sample - loss: 8.0739 - val_loss: 8.1530\n",
      "Epoch 28/1000\n",
      "6815/6815 [==============================] - 2s 304us/sample - loss: 8.0711 - val_loss: 8.1411\n",
      "Epoch 29/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0691 - val_loss: 8.1308\n",
      "Epoch 30/1000\n",
      "6815/6815 [==============================] - 2s 309us/sample - loss: 8.0689 - val_loss: 8.1188\n",
      "Epoch 31/1000\n",
      "6815/6815 [==============================] - 2s 317us/sample - loss: 8.0480 - val_loss: 8.1228\n",
      "Epoch 32/1000\n",
      "6815/6815 [==============================] - 2s 313us/sample - loss: 8.0424 - val_loss: 8.0906\n",
      "Epoch 33/1000\n",
      "6815/6815 [==============================] - 2s 314us/sample - loss: 8.0334 - val_loss: 8.0998\n",
      "Epoch 34/1000\n",
      "6815/6815 [==============================] - 2s 312us/sample - loss: 8.0471 - val_loss: 8.1003\n",
      "Epoch 35/1000\n",
      "6815/6815 [==============================] - 2s 309us/sample - loss: 8.0374 - val_loss: 8.1115\n",
      "Epoch 36/1000\n",
      "6815/6815 [==============================] - 2s 310us/sample - loss: 8.0303 - val_loss: 8.0998\n",
      "Epoch 37/1000\n",
      "6815/6815 [==============================] - 2s 304us/sample - loss: 8.0411 - val_loss: 8.1196\n",
      "Epoch 38/1000\n",
      "6815/6815 [==============================] - 2s 309us/sample - loss: 8.0349 - val_loss: 8.0915\n",
      "Epoch 39/1000\n",
      "6815/6815 [==============================] - 2s 294us/sample - loss: 8.0358 - val_loss: 8.0846\n",
      "Epoch 40/1000\n",
      "6815/6815 [==============================] - 2s 297us/sample - loss: 8.0316 - val_loss: 8.0928\n",
      "Epoch 41/1000\n",
      "6815/6815 [==============================] - 2s 300us/sample - loss: 8.0305 - val_loss: 8.0985\n",
      "Epoch 42/1000\n",
      "6815/6815 [==============================] - 2s 303us/sample - loss: 8.0284 - val_loss: 8.0969\n",
      "Epoch 43/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.0321 - val_loss: 8.0878\n",
      "Epoch 44/1000\n",
      "6815/6815 [==============================] - 2s 306us/sample - loss: 8.0321 - val_loss: 8.1094\n",
      "Epoch 45/1000\n",
      "6815/6815 [==============================] - 2s 303us/sample - loss: 8.0309 - val_loss: 8.1107\n",
      "Epoch 46/1000\n",
      "6815/6815 [==============================] - 2s 300us/sample - loss: 8.0304 - val_loss: 8.0926\n",
      "Epoch 47/1000\n",
      "6815/6815 [==============================] - 2s 303us/sample - loss: 8.0265 - val_loss: 8.0882\n",
      "Epoch 48/1000\n",
      "6815/6815 [==============================] - 2s 301us/sample - loss: 8.0250 - val_loss: 8.1028\n",
      "Epoch 49/1000\n",
      "6815/6815 [==============================] - 2s 315us/sample - loss: 8.0313 - val_loss: 8.0887\n",
      "Epoch 50/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.0312 - val_loss: 8.1195\n",
      "Epoch 51/1000\n",
      "6815/6815 [==============================] - 2s 303us/sample - loss: 8.0281 - val_loss: 8.0977\n",
      "Epoch 52/1000\n",
      "6815/6815 [==============================] - 2s 310us/sample - loss: 8.0341 - val_loss: 8.1154\n",
      "Epoch 53/1000\n",
      "6815/6815 [==============================] - 2s 305us/sample - loss: 8.0312 - val_loss: 8.0815\n",
      "Epoch 54/1000\n",
      "6815/6815 [==============================] - 2s 300us/sample - loss: 8.0274 - val_loss: 8.1048\n",
      "Epoch 55/1000\n",
      "6815/6815 [==============================] - 2s 302us/sample - loss: 8.0296 - val_loss: 8.0891\n",
      "Epoch 56/1000\n",
      "6815/6815 [==============================] - 2s 306us/sample - loss: 8.0269 - val_loss: 8.0929\n",
      "Epoch 57/1000\n",
      "6815/6815 [==============================] - 2s 320us/sample - loss: 8.0226 - val_loss: 8.0862\n",
      "Epoch 58/1000\n",
      "6815/6815 [==============================] - 2s 309us/sample - loss: 8.0274 - val_loss: 8.0876\n",
      "Epoch 59/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.0305 - val_loss: 8.1020\n",
      "Epoch 60/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.0218 - val_loss: 8.1128\n",
      "Epoch 61/1000\n",
      "6815/6815 [==============================] - 2s 314us/sample - loss: 8.0288 - val_loss: 8.1097\n",
      "Epoch 62/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0234 - val_loss: 8.0720\n",
      "Epoch 63/1000\n",
      "6815/6815 [==============================] - 2s 316us/sample - loss: 8.0241 - val_loss: 8.0950\n",
      "Epoch 64/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0208 - val_loss: 8.0906\n",
      "Epoch 65/1000\n",
      "6815/6815 [==============================] - 2s 309us/sample - loss: 8.0285 - val_loss: 8.0915\n",
      "Epoch 66/1000\n",
      "6815/6815 [==============================] - 2s 314us/sample - loss: 8.0235 - val_loss: 8.1091\n",
      "Epoch 67/1000\n",
      "6815/6815 [==============================] - 2s 308us/sample - loss: 8.0246 - val_loss: 8.0947\n",
      "Epoch 68/1000\n",
      "6815/6815 [==============================] - 2s 313us/sample - loss: 8.0286 - val_loss: 8.0788\n",
      "Epoch 69/1000\n",
      "6815/6815 [==============================] - 2s 306us/sample - loss: 8.0230 - val_loss: 8.0863\n",
      "Epoch 70/1000\n",
      "6815/6815 [==============================] - 2s 300us/sample - loss: 8.0230 - val_loss: 8.0780\n",
      "Epoch 71/1000\n",
      "6815/6815 [==============================] - 2s 295us/sample - loss: 8.0198 - val_loss: 8.0769\n",
      "Epoch 72/1000\n",
      "6815/6815 [==============================] - 2s 302us/sample - loss: 8.0200 - val_loss: 8.0717\n",
      "Epoch 73/1000\n",
      "6815/6815 [==============================] - 2s 306us/sample - loss: 8.0173 - val_loss: 8.1080\n",
      "Epoch 74/1000\n",
      "6815/6815 [==============================] - 2s 300us/sample - loss: 8.0237 - val_loss: 8.0773\n",
      "Epoch 75/1000\n",
      "6815/6815 [==============================] - 2s 295us/sample - loss: 8.0239 - val_loss: 8.1192\n",
      "Epoch 76/1000\n",
      "6815/6815 [==============================] - 2s 298us/sample - loss: 8.0261 - val_loss: 8.0757\n",
      "Epoch 77/1000\n",
      "6815/6815 [==============================] - 2s 295us/sample - loss: 8.0212 - val_loss: 8.0701\n",
      "Epoch 78/1000\n",
      "6815/6815 [==============================] - 2s 300us/sample - loss: 8.0192 - val_loss: 8.0971\n",
      "Epoch 79/1000\n",
      "6815/6815 [==============================] - 2s 297us/sample - loss: 8.0191 - val_loss: 8.1033\n",
      "Epoch 80/1000\n",
      "6815/6815 [==============================] - 2s 295us/sample - loss: 8.0261 - val_loss: 8.0899\n",
      "Epoch 81/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0174 - val_loss: 8.0861\n",
      "Epoch 82/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0210 - val_loss: 8.0817\n",
      "Epoch 83/1000\n",
      "6815/6815 [==============================] - 2s 307us/sample - loss: 8.0195 - val_loss: 8.0906\n",
      "Epoch 84/1000\n",
      "6815/6815 [==============================] - 2s 301us/sample - loss: 8.0246 - val_loss: 8.0767\n",
      "Epoch 85/1000\n",
      "6815/6815 [==============================] - 2s 302us/sample - loss: 8.0169 - val_loss: 8.0884\n",
      "Epoch 86/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0209 - val_loss: 8.0916\n",
      "Epoch 87/1000\n",
      "6815/6815 [==============================] - 2s 310us/sample - loss: 8.0234 - val_loss: 8.0802\n",
      "Epoch 88/1000\n",
      "6815/6815 [==============================] - 2s 320us/sample - loss: 8.0278 - val_loss: 8.0713\n",
      "Epoch 89/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0173 - val_loss: 8.1009\n",
      "Epoch 90/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0227 - val_loss: 8.0703\n",
      "Epoch 91/1000\n",
      "6815/6815 [==============================] - 2s 308us/sample - loss: 8.0229 - val_loss: 8.0934\n",
      "Epoch 92/1000\n",
      "6815/6815 [==============================] - 2s 309us/sample - loss: 8.0224 - val_loss: 8.0908\n",
      "Epoch 93/1000\n",
      "6815/6815 [==============================] - 2s 313us/sample - loss: 8.0201 - val_loss: 8.0749\n",
      "Epoch 94/1000\n",
      "6815/6815 [==============================] - 2s 309us/sample - loss: 8.0224 - val_loss: 8.0948\n",
      "Epoch 95/1000\n",
      "6815/6815 [==============================] - 2s 311us/sample - loss: 8.0250 - val_loss: 8.0772\n",
      "Epoch 96/1000\n",
      "6815/6815 [==============================] - 2s 304us/sample - loss: 8.0205 - val_loss: 8.0800\n",
      "Epoch 97/1000\n",
      "6815/6815 [==============================] - 2s 322us/sample - loss: 8.0198 - val_loss: 8.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:32:03,518 - INFO - Weights saved to weights/SCEDC_30_neural_neural_neural\n",
      "2025-07-23 18:32:03,519 - INFO - Test results: LL_ts=-5.940391540527344, LL_t=1.7972055673599243, LL_s=-7.7375969886779785\n",
      "2025-07-23 18:32:03,662 - INFO - Results saved to csv/SCEDC_30_neural_neural_neural.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# datasets_id should be in [\"ComCat\", \"SaltonSea\", \"SanJac\", \"WHITE\", \"SCEDC_20\", \"SCEDC_25\", \"SCEDC_30\"]\n",
    "\n",
    "datasets_id = \"SCEDC_30\"\n",
    "\n",
    "\n",
    "######## ETAS ########\n",
    "# temporal_id = \"empirical\"\n",
    "# spatial_id = \"empirical\"\n",
    "# kappa_id = \"empirical\"\n",
    "######## ETAS ########\n",
    "\n",
    "######## NKF ########\n",
    "temporal_id = \"neural\"\n",
    "spatial_id = \"neural\"\n",
    "kappa_id = \"neural\"\n",
    "######## NKF ########\n",
    "\n",
    "# You can also try different combinations.\n",
    "# such as:\n",
    "# temporal_id = \"neural\"\n",
    "# spatial_id = \"neural\"\n",
    "# kappa_id = \"empirical\"\n",
    "\n",
    "\n",
    "if datasets_id not in DATASET_CONFIGS:\n",
    "    logger.error(f\"Unknown dataset: {datasets_id}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "config = DATASET_CONFIGS[datasets_id]\n",
    "\n",
    "try:\n",
    "    raw_catalog = pd.read_csv(config[\"catalog_path\"])\n",
    "    cat_shape = np.load(config[\"shape_path\"])\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Failed to load dataset files: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Filter catalog for SCEDC datasets\n",
    "if datasets_id in [\"SCEDC_20\", \"SCEDC_25\", \"SCEDC_30\"]:\n",
    "    min_magnitude = float(datasets_id.split(\"_\")[-1]) / 10\n",
    "    raw_catalog = raw_catalog[raw_catalog['magnitude'] >= min_magnitude].reset_index(drop=True)\n",
    "\n",
    "# Split data by time\n",
    "auxiliary_num = len(raw_catalog[\n",
    "    (raw_catalog['time_days'] >= config[\"auxiliary_start\"]) & \n",
    "    (raw_catalog['time_days'] < config[\"training_start\"])\n",
    "])\n",
    "training_num = len(raw_catalog[\n",
    "    (raw_catalog['time_days'] >= config[\"training_start\"]) & \n",
    "    (raw_catalog['time_days'] < config[\"validation_start\"])\n",
    "])\n",
    "validation_num = len(raw_catalog[\n",
    "    (raw_catalog['time_days'] >= config[\"validation_start\"]) & \n",
    "    (raw_catalog['time_days'] < config[\"testing_start\"])\n",
    "])\n",
    "testing_num = len(raw_catalog[\n",
    "    (raw_catalog['time_days'] >= config[\"testing_start\"]) & \n",
    "    (raw_catalog['time_days'] <= config[\"testing_end\"])\n",
    "])\n",
    "\n",
    "logger.info(f\"{datasets_id}: auxiliary={auxiliary_num}, training={training_num}, \"\n",
    "            f\"validation={validation_num}, testing={testing_num}\")\n",
    "\n",
    "input_dim_train = auxiliary_num\n",
    "data_t_train = raw_catalog['time_days'].values[auxiliary_num - input_dim_train: auxiliary_num + training_num]\n",
    "data_m_train = raw_catalog['magnitude'].values[auxiliary_num - input_dim_train: auxiliary_num + training_num]\n",
    "data_x_train = raw_catalog['x'].values[auxiliary_num - input_dim_train: auxiliary_num + training_num]\n",
    "data_y_train = raw_catalog['y'].values[auxiliary_num - input_dim_train: auxiliary_num + training_num]\n",
    "\n",
    "input_dim_val = auxiliary_num\n",
    "data_t_val = raw_catalog['time_days'].values[\n",
    "    auxiliary_num + training_num - input_dim_val: auxiliary_num + training_num + validation_num\n",
    "]\n",
    "data_m_val = raw_catalog['magnitude'].values[\n",
    "    auxiliary_num + training_num - input_dim_val: auxiliary_num + training_num + validation_num\n",
    "]\n",
    "data_x_val = raw_catalog['x'].values[\n",
    "    auxiliary_num + training_num - input_dim_val: auxiliary_num + training_num + validation_num\n",
    "]\n",
    "data_y_val = raw_catalog['y'].values[\n",
    "    auxiliary_num + training_num - input_dim_val: auxiliary_num + training_num + validation_num\n",
    "]\n",
    "\n",
    "input_dim_test = auxiliary_num\n",
    "data_t_test = raw_catalog['time_days'].values[\n",
    "    auxiliary_num + training_num + validation_num - input_dim_test: \n",
    "    auxiliary_num + training_num + validation_num + testing_num\n",
    "]\n",
    "data_m_test = raw_catalog['magnitude'].values[\n",
    "    auxiliary_num + training_num + validation_num - input_dim_test: \n",
    "    auxiliary_num + training_num + validation_num + testing_num\n",
    "]\n",
    "data_x_test = raw_catalog['x'].values[\n",
    "    auxiliary_num + training_num + validation_num - input_dim_test: \n",
    "    auxiliary_num + training_num + validation_num + testing_num\n",
    "]\n",
    "data_y_test = raw_catalog['y'].values[\n",
    "    auxiliary_num + training_num + validation_num - input_dim_test: \n",
    "    auxiliary_num + training_num + validation_num + testing_num\n",
    "]\n",
    "\n",
    "center_latitude = raw_catalog['latitude'].mean()\n",
    "center_longitude = raw_catalog['longitude'].mean()\n",
    "cat_shape_x, cat_shape_y = azimuthal_equidistant_projection(\n",
    "    cat_shape[:, 0], cat_shape[:, 1], center_latitude, center_longitude\n",
    ")\n",
    "cat_shape_xy = np.stack((cat_shape_x, cat_shape_y), axis=1)\n",
    "obj_area = quadrilateral_area(cat_shape_xy)\n",
    "\n",
    "model = KernelPointProcess(\n",
    "    time_step_train=input_dim_train,\n",
    "    time_step_val=input_dim_val,\n",
    "    time_step_test=input_dim_test,\n",
    "    temporal_id=temporal_id,\n",
    "    spatial_id=spatial_id,\n",
    "    kappa_id=kappa_id,\n",
    "    global_m0=config[\"global_m0\"],\n",
    "    area=obj_area,\n",
    "    size_layer=5,\n",
    "    size_nn=32\n",
    ").set_train_data(\n",
    "    data_t_train, data_m_train, data_x_train, data_y_train\n",
    ").set_val_data(\n",
    "    data_t_val, data_m_val, data_x_val, data_y_val\n",
    ").set_test_data(\n",
    "    data_t_test, data_m_test, data_x_test, data_y_test\n",
    ").set_model().compile().fit_eval(\n",
    "    epochs=1000, batch_size=128\n",
    ").eval_train().eval_val().eval_test().save_weights(\n",
    "    f\"weights/{datasets_id}_{temporal_id}_{spatial_id}_{kappa_id}\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Test results: LL_ts={model.LL_ts_average_test}, \"\n",
    "            f\"LL_t={model.LL_t_average_test}, LL_s={model.LL_s_average_test}\")\n",
    "\n",
    "try:\n",
    "    with open(\"ll_seismic_model.log\", \"a\") as log_file:\n",
    "        log_file.write(\n",
    "            f\"{datasets_id}, {temporal_id}, {spatial_id}, {kappa_id}, \"\n",
    "            f\"{model.LL_ts_average_train}, {model.LL_t_average_train}, {model.LL_s_average_train}, \"\n",
    "            f\"{model.LL_ts_average_val}, {model.LL_t_average_val}, {model.LL_s_average_val}, \"\n",
    "            f\"{model.LL_ts_average_test}, {model.LL_t_average_test}, {model.LL_s_average_test}\\n\"\n",
    "        )\n",
    "except IOError as e:\n",
    "    logger.error(f\"Failed to write to log file: {e}\")\n",
    "\n",
    "results_catalog = raw_catalog.copy()\n",
    "results_catalog.loc[auxiliary_num: auxiliary_num + training_num - 1, 'step'] = 'train'\n",
    "results_catalog.loc[auxiliary_num + training_num: auxiliary_num + training_num + validation_num - 1, 'step'] = 'val'\n",
    "results_catalog.loc[\n",
    "    auxiliary_num + training_num + validation_num: \n",
    "    auxiliary_num + training_num + validation_num + testing_num - 1, 'step'\n",
    "] = 'test'\n",
    "\n",
    "for dataset, logli_t, logli_s, logli_ts, loglam_t, loglam_ts, intlam in [\n",
    "    ('train', model.LL_t_train, model.LL_s_train, model.LL_ts_train, np.log(model.lam_t_train), np.log(model.lam_ts_train), model.Int_lam_train),\n",
    "    ('val', model.LL_t_val, model.LL_s_val, model.LL_ts_val, np.log(model.lam_t_val), np.log(model.lam_ts_val), model.Int_lam_val),\n",
    "    ('test', model.LL_t_test, model.LL_s_test, model.LL_ts_test, np.log(model.lam_t_test), np.log(model.lam_ts_test), model.Int_lam_test)\n",
    "]:\n",
    "    start_idx = {'train': auxiliary_num, 'val': auxiliary_num + training_num, 'test': auxiliary_num + training_num + validation_num}[dataset]\n",
    "    end_idx = start_idx + {'train': training_num, 'val': validation_num, 'test': testing_num}[dataset] - 1\n",
    "    results_catalog.loc[start_idx:end_idx, 'logli_t'] = logli_t.flatten()\n",
    "    results_catalog.loc[start_idx:end_idx, 'logli_s'] = logli_s.flatten()\n",
    "    results_catalog.loc[start_idx:end_idx, 'logli_ts'] = logli_ts.flatten()\n",
    "    results_catalog.loc[start_idx:end_idx, 'loglam_t'] = loglam_t.flatten()\n",
    "    results_catalog.loc[start_idx:end_idx, 'loglam_ts'] = loglam_ts.flatten()\n",
    "    results_catalog.loc[start_idx:end_idx, 'intlam'] = intlam.flatten()\n",
    "\n",
    "try:\n",
    "    results_catalog.to_csv(\n",
    "        f\"csv/{datasets_id}_{temporal_id}_{spatial_id}_{kappa_id}.csv\",\n",
    "        index=False,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    logger.info(f\"Results saved to csv/{datasets_id}_{temporal_id}_{spatial_id}_{kappa_id}.csv\")\n",
    "except IOError as e:\n",
    "    logger.error(f\"Failed to save results to CSV: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f6b76-9cf5-47bd-a7a9-e068b8e4e420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zcx-py311-tf",
   "language": "python",
   "name": "zcx-py311-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
